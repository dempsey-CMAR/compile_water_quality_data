---
title: "CMAR CMP Data Processing App"
date: "version 0.0.9000"
output:
  flexdashboard::flex_dashboard:
    orientation: rows
runtime: shiny
---

<style type="text/css">
  body {font-size: 16px;}
</style>

<style>
  .nav-tabs-custom > .nav-tabs > li.active {border-top-color: #063e4d}
</style>

<style>
.nav-tabs-custom .nav-tabs li.active a {
  color: #063e4d;
  font-size: 16px;
}

.nav-tabs-custom .nav-tabs li:not(.active) a {
  color: grey;
  font-size: 16px;
}
</style>

```{css table-style, echo = FALSE}
#section-example-metadata-log
.chart-shim {
overflow-y: scroll;
}
```

```{r setup, echo=FALSE, include=FALSE}
library(data.table)
library(dplyr)
library(DT)
library(ggplot2)
library(here)
library(leaflet)
library(lubridate)
library(plotly)
library(readr)
library(sensorstrings)
library(shiny)
library(stringr)
library(tidyr)

```

```{r}
#########################################
###### Reactive UI ###################### 
#########################################

button_style <- "color: #fff; background-color: #063e4d; border-color: #063e4d"

output$clear_button <- renderUI({
   actionButton(
    "clear_data", "Refresh App", icon("arrows-rotate"),
    style = button_style,
    width = '100%'
  )
})

output$compile_button <- renderUI({
  req(input$upload)
  
  actionButton(
    "compile", "Compile Data", 
    width = '100%',
    style = button_style,
    onclick = "location.href='#section-preview';")
})

output$preview_button <- renderUI({
  req(input$upload)
  
  textInput(
    "preview_window",
    "Preview Window (days)",
    value = 2, width = '50%'
  )
})

output$trim_start <- renderUI({
  dat <- compile_data()
  textInput(
    "ts_start", "First Good Timestamp",
    value = min(dat$timestamp_utc),
    width = '100%',
    placeholder = "yyyy-mm-dd hh:mm:ss"
  )
})

output$trim_end <- renderUI({
  dat <- compile_data()
  textInput(
    "ts_end", "Last Good Timestamp",
    value = max(dat$timestamp_utc),
    width = '100%',
    placeholder = "yyyy-mm-dd hh:mm:ss"
  )
})

output$trim_button <- renderUI({
  req(input$compile)
  actionButton(
    "apply_trim", 
    "Trim Data", 
    width = '100%',
    style = button_style,
    onclick = "location.href='#section-trimmed-data';")
})

output$download_button <- renderUI({
  req(input$apply_trim)
  
  downloadButton(
    "download_data", 
    "Download Data",
    style = paste0(button_style, "; width: 100%;")
  )
})

output$download_data <- downloadHandler(
  filename = function() {
    paste0(get_folder_name(), ".csv")
  },
  content = function(file) {
    fwrite(
      trim_data() %>% 
        mutate(timestamp_utc = format(timestamp_utc)), file)
  }
)

#########################################
###### Data Wrangling ################### 
#########################################

get_files <- reactive({
  req(input$upload)
  
  files <- unzip(input$upload$datapath, list = FALSE)
  
  validate(need(length(files) > 0, "No csv files found in zipped folder"))
 
  files <- files[grep(".csv|.xlsx", files)]
  files <- files[grep("other_vemco", files, invert = TRUE)]
  
  return(files)
})

get_folder_name <- reactive({
  req(input$upload)

  files <- get_files()

  files <- sub("./", "", files[1])

  folder <- gsub("(.+?)(\\/.*)", "\\1", files)

  return(folder)
})

read_log <- reactive({
  req(input$upload)
  
  zip_files <- get_files()
  path <- zip_files[grep("log", zip_files)]
  
  ss_log <- ss_read_log(path, parse = FALSE, verbose = FALSE)
  
  return(ss_log)
})

compile_data <- eventReactive(input$compile, {
  
  zip_files <- get_files()
  
  depl_log <- read_log()
  
  # do log check here so that log will still be displayed in Log tab,
  # BUT code will not attempt to compile data
  log_cols <- colnames(depl_log)
  cols <- c("deployment_date", "retrieval_date", "deployment_latitude", "deployment_longitude", "sensor_type", "sensor_serial_number")
  
  col_check <- all(cols %in% log_cols)
  missing_cols <- cols[which(!(cols %in% log_cols))]
  
  validate(need(isTRUE(col_check), paste("ERROR: Required column(s) not found in log:", paste(missing_cols, collapse = ", "))))
  
  depl_log <- depl_log %>% ss_parse_log(verbose = FALSE)
  depl_dates <- depl_log$deployment_dates
  sn_table <- depl_log$sn_table
  
  depl_data <- tibble(NULL)
  
  sn_am <- sn_table %>%
    filter(str_detect(log_sensor, regex("aquameasure", ignore_case = TRUE)))
  if(nrow(sn_am) > 0) {
    am <- ss_compile_aquameasure_data(
      path = zip_files[grep("aquameasure", zip_files)],
      sn_table = sn_table,
      deployment_dates = depl_dates,
      trim = TRUE
    )
    depl_data <- bind_rows(depl_data, am)
  }
  
  sn_hobo <- sn_table %>%
    filter(str_detect(log_sensor, regex("hobo", ignore_case = TRUE)))
  if(nrow(sn_hobo) > 0) {
    hobo <- ss_compile_hobo_data(
      path = zip_files[grep("hobo", zip_files)],
      sn_table = sn_table,
      deployment_dates = depl_dates,
      trim = TRUE
    )
    depl_data <- bind_rows(depl_data, hobo)
  }
  
  sn_tidbit <- sn_table %>%
    filter(str_detect(log_sensor, regex("tidbit", ignore_case = TRUE)))
  if (nrow(sn_tidbit) > 0) {
    tidbit <- ss_compile_hobo_data(
      path = zip_files[grep("tidbit", zip_files)],
      sn_table = sn_tidbit,
      deployment_dates = depl_dates,
      trim = TRUE,
      sensor_make = "tidbit"
    )
    depl_data <- bind_rows(depl_data, tidbit)
  }
  
  sn_vem <- sn_table %>%
    filter(str_detect(log_sensor, regex("VR2AR", ignore_case = TRUE)))
  if (nrow(sn_vem) > 0) {
    vemco <- ss_compile_vemco_data(
      path = zip_files[grep("vemco", zip_files)],
      sn_table = sn_vem,
      deployment_dates = depl_dates,
      trim = TRUE
    )
    depl_data <- bind_rows(depl_data, vemco)
  }
  
  return(depl_data)
})


observeEvent(input$compile, {
  showModal(
    modalDialog(
      "Compiling data. This may take several minutes",
      easyClose = TRUE
    ))
})  

observeEvent(input$apply_trim, {
  showModal(
    modalDialog(
      "Plotting data. This may take several minutes",
      easyClose = TRUE
    ))
})  


trim_data <- eventReactive(input$apply_trim, {
  ts_min <- as_datetime(input$ts_start)
  ts_max <- as_datetime(input$ts_end)
  
  validate(need(!is.na(ts_min), "Fix Timestamp: the timestamp entered into the First Good Timestamp field must be in the order yyyy-mm-dd hh:mm:ss"))
  validate(need(!is.na(ts_max), "Fix Timestamp: the timestamp entered into the Last Good Timestamp field must be in the order yyyy-mm-dd hh:mm:ss"))
  validate(need(ts_max > ts_min, "Fix Timestamps: the timestamp entered in the First Good Timestamp field must occur before the timestamp entered in the Last Good Timestamp field"))
    
  dat <- compile_data() %>% 
    filter(timestamp_utc >= ts_min, timestamp_utc <= ts_max)

  return(dat)
})

observeEvent(input$clear_data, {
  session$reload()
})

#########################################
###### Figures & Tables ################## 
#########################################

click_start <- reactive({
  req(input$compile)
  
  ts_info <- event_data("plotly_click", source = "plot1")
  
  if (is.null(ts_info)) {
      "Click a point from the chart above to print its timestamp. Double click the chart to clear."
  } else {
    ts_new <- data.frame(ts = as_datetime(ts_info$x))
    ts_new$ts <- format(ts_new$ts, "%Y-%m-%d %H:%M:%S")
    
    ts_new$ts
  }
  
})

click_end <- reactive({
  req(input$compile)
  
  ts_info <- event_data("plotly_click", source = "plot2")
  
  if (is.null(ts_info)) {
      "Click a point from the chart above to print its timestamp. Double click the chart to clear."
  } else {
    ts_new <- data.frame(ts = as_datetime(ts_info$x))
    ts_new$ts <- format(ts_new$ts, "%Y-%m-%d %H:%M:%S")
    
    ts_new$ts
  }
  
})

```


Column {.sidebar data-width=350}
=======================================

```{r}
uiOutput("clear_button")

fileInput(
  "upload",
  "",
  accept = ".zip",
  buttonLabel = "Upload Zip Folder",
  multiple = FALSE,
  width = '100%'
)

uiOutput("compile_button")
HTML("<br>")

uiOutput("preview_button")

uiOutput("trim_start")

uiOutput("trim_end")

uiOutput("trim_button")
HTML("<br>")

uiOutput("download_button")

```

Instructions {data-icon="fa-circle-info"}
=======================================

Col {.tabset}
-------------------------------------

### Overview

Welcome to the [Centre for Marine Applied Research](https://cmar.ca/)'s (CMAR) Data Processing App. 

The app provides a user interface to compile and trim data collected through the Water Quality Branch of CMAR's [Coastal Monitoring Program](https://cmar.ca/coastal-monitoring-program/).

Please follow the instructions in the tabs above to compile the data.

**Pay close attention the file format, folder structure, and metadata requirements, or else the app WILL NOT run as intended.**

If you have followed all instructions and the app still does not run, please email info\<at\>cmar.ca with the subject line "CMAR CMP Data Processing App Issue". Include a copy of your zip folder and a screenshot of any error messages.

### **Step 1: Upload Zipped Data**

The data from a single deployment **must** be saved in a specific format and folder structure for the app to access.

- A separate data file must be offloaded from each sensor and saved in .csv format.
- The .csv files offloaded from hobo and tidbit sensors must be saved in a folder called "hobo".
- The .csv files downloaded from aquameasure sensors must be saved in a folder named "aquameasure". These files must be downloaded from the aquaMeasure app using the "Device Records" selection from the "Format Options" menu.
- The .csv files offloaded from VR2AR and VR2AR-X sensors must be saved in a folder named "vemco".

A metadata log with information about the deployment must be saved in a folder called "log". The log must be saved as a .csv, .xls, or .xlsx file. It must include a row for each sensor and the following columns:

- **deployment_date**: the date the deployment started, in ISO-8601 format, i.e. yyyy-mm-dd. Example: 2025-03-07. This must be the same for each row.
- **retrieval_date**: the date the deployment ended, in ISO-8601 format, i.e. yyyy-mm-dd. Example: 2025-03-10. This must be the same for each row.
- **deployment_latitude**: the latitude of the deployed sensors (in decimal-degrees). Must be the same for each row.
- **deployment_longitude**: the longitude of the deployed sensors (in decimal-degrees). Must be the same for each row.
- **sensor_type**: the manufacturer of the sensor. Entries must include "aquameasure", "hobo", "tidbit", or "vr2ar". Example: "aquaMeasure DOT", "HOBO Pro V2".
- **sensor_serial_number**: serial number of the sensor.
- **sensor_depth_m**: the estimated depth of the sensor at low tide, in units of metres.

See the Example Metadata Log tab to view and download a log template.

The log, aquameasure, hobo, tidbit, and/or vemco folders must all be saved in the same folder (e.g., deployment_data). This folder must be saved as a zip file and may be uploaded to the app using the "Upload Zip File" button. **This folder must only contain data from sensors that were on the same sensor string (e.g., deployed at the same location for the same deployment and retrieval dates)**.

When the zip folder has been successfully uploaded, the [Metadata](http://127.0.0.1:4385/compile_app.Rmd#section-metadata) page will be populated with the deployment log, a map showing the deployment location, and a list of uploaded files.

Note: the name of the zip folder must not contain the strings "hobo", "aquameasure", "vemco", or "tidbit".


### **Step 2: Compile Data**

After the zip file with the deployment data has been successfully uploaded, a "Compile Data" button and "Preview Window (days)" will appear in the sidebar.

**Note**: for some reason, the first time the "Compile" button is clicked, the App will refresh. You will need to re-upload the zip folder and click "Compile" again.

Several things happen when the "Compile Data" button is clicked:

1. The app will automatically open the "Preview" page.
2. A popup message will appear to signal that the data is being compiled.
3. Behind the scenes, the data from the different sensors will be compiled into a single data frame with additional metadata columns. Depending on the size of the data files, this could take a few minutes with little feedback. Be patient.
4. Additional input boxes and the "Trim Data" button will appear in the sidebar.

After the data has been compiled, the Deployment Start and Deployment End figures will be generated. These are interactive figures displaying the data from the beginning or end of the deployment, coloured by sensor_depth_at_low_tide_m. The "Preview Window (days)" input controls how many days of data are plotted. The default (2 days) should be sufficient for most applications, but can be adjusted to provide more context.

- Hover over points to see information including the timestamp, measurement value, sensor type, and sensor serial number. 
- Click on entries in the legend to add/remove data series. 
- Click and drag to zoom in on interesting features.
- Download image by clicking the camera icon that appears in the top right of the figure.  

See **Step 3: Trim Data** for explanation of the input boxes, Trim button, and "Print Timestamp" panels.

### **Step 3: Trim Data**

The compiled data may include observations from before or after the deployment that should not be included in the downloaded data. These can be trimmed out of the dataset in this step.

Use the Deployment Start figure to identify the timestamp of the first "Good" observation, i.e., the first data point that should be included. It may be helpful to zoom in on the figure to do so. Double click to zoom back out. Clicking a data point will print its timestamp to the "Print Start Timestamp" panel. This can be copied and pasted into the "First Good Timestamp" text box in the side bar. Adjust the "Preview Window (days)" to show more or less days of data if required.

Use the Deployment End figure to find the last good observation, and paste its timestamp in the "Last Good Timestamp" text box.

Click the "Trim Data" button. This will remove the observations before/after the first and last good observation. The "Trimmed Data" page will open and the tabs will be populated:

- **Figure**: a static figure displaying the data from all sensors in the validation trimmed to the specified dates and coloured by sensor_depth_at_low_tide_m.
- **Data**: a table of trimmed data. 
  - Search in the table using the "Search" bar.
  - Reorder the rows based on any column using the arrow in the column header.
  
### **Step 4: Download Data**

After the "Trim Data" button is clicked, a "Download Data" button will appear.

Click this to download the trimmed dataset. The file will be in a csv format and named after the original zip folder.

### Example Metadata Log

The example metadata log below illustrates the required columns for the compile app. Other columns may be included, but will not be used in the compile code. 

Click the "CSV" button below to download the example log to use as a template.

Recall that the log must:

- be a csv, xls, or xlsx file.
- be saved in a folder named "log".
- include a row for each sensor validated.
- include columns named exactly "deployment_date", "retrieval_date", "deployment_latitude", "deployment_longitude", "sensor_type", and "sensor_serial_number".

\br

```{r}

log_df <- data.frame(
  deployment_date = rep("2024-04-08", 4),
  retrieval_date = rep("2025-03-12", 4),
  deployment_latitude = rep(44.664, 4),
  deployment_longitude = rep(63.560, 4),
  sensor_type = c("hobo", "aquameasure", "hobo", "vr2ar"),
  sensor_serial_number = c(21465779, 671022, 21488196, 548562),
  sensor_depth_m = c(2, 5, 10, 15)
)

datatable(
  log_df,
  rownames = FALSE,
  extensions = 'Buttons',
  options = list(
    dom = 'Bft',
    paging = FALSE,
    searching = FALSE,
    buttons = list(
      list(extend = 'csv', title = "example_deployment_log") 
    ),
    columnDefs = list(list(className = 'dt-center', targets = "_all"))
  )
)

```



Metadata {data-orientation=rows data-icon="fa-location-dot"}
=======================================

Col {.tabset data-height=100}
-------------------------------------

### Notes
- The Deployment Log tab displays the deployment metadata, as read in from the the log file.
- The Map tab shows the deployment location, based on deployment_latitude and deployment_longitude entered in the log.
- The Uploaded Files tab lists the files that were uploaded from the zip folder.


```{r}
# st_location <- data.frame(
#   station = depl_station,
#   latitude = depl_info$deployment_latitude,
#   longitude = depl_info$deployment_longitude,
#   retrieval_latitude = depl_info$retrieval_latitude,
#   retrieval_longitude = depl_info$retrieval_longitude
# )
# 
# #ss_check_station_radius(st_location) # checks against water quality stations
# ss_check_station_in_ocean(st_location) # slow because reads in shape file
# ss_check_station_drift(st_location, max_drift = 0)

```


Col {.tabset}
-------------------------------------

### Deployment Log

```{r}
# renderDataTable is deprecated for renderDT
# BUT renderDT does not add scroll bars to the table
renderDataTable({
  read_log() %>%
    datatable(
      rownames = FALSE,
      options = list(
        dom = 'ft',
        paging = FALSE,
        searching = TRUE,
        fillContainer = TRUE,
        scrollX = "500px",
        columnDefs = list(list(className = 'dt-center', targets = "_all"))))
})
```

### Map
```{r}
output$st_map <- renderLeaflet({
  
  st_location <- read_log() %>% 
    select(deployment_latitude, deployment_longitude, station) 
  
  leaflet(st_location) %>%
    addProviderTiles(providers$CartoDB.Positron) %>%
    addCircleMarkers(
      data = st_location,
      lng = ~deployment_longitude, 
      lat = ~deployment_latitude, 
      label = ~station,
      weight = 1, fillOpacity = 0.75, radius = 5
    ) %>%
    addScaleBar(
      position = "bottomleft",
      options = scaleBarOptions(imperial = FALSE)
    )
})

leafletOutput("st_map")  
```

### Uploaded Files
```{r}
renderPrint({
  validate(need(input$upload, "No files uploaded"))
  get_files()
})
```

Preview {data-icon="fa-magnifying-glass"}
=======================================

Col 
-------------------------------------

### Deployment Start
```{r}
output$p_start <- renderPlotly({
  req(input$preview_window)
  validate(need(input$compile, "No data to plot"))
  
  n_days <- as.numeric(input$preview_window)
  
  validate(need(!is.na(n_days), "Problem with the Preview Window. Please enter a numeric value."))
  
  dat <- compile_data() 
  
  p <- dat %>% 
    filter(timestamp_utc < min(dat$timestamp_utc) + days(n_days)) %>% 
    ss_ggplot_variables(
      point_size = 1, axis_label_newline = FALSE
    )
  
  p <- ggplotly(p, tooltip = "text", source = "plot1") %>%
    config(
      modeBarButtonsToRemove = list("toggleSpikelines", "autoScale2d"),
      displaylogo = FALSE,
      toImageButtonOptions = list(
        format = "png",
        filename = "deployment_start",
        width = 900, height = 500
      )) 
  p
})

plotlyOutput("p_start")

```

### Deployment End
```{r}
output$p_end <- renderPlotly({
  
  validate(need(input$compile, "No data to plot"))
  
  n_days <- as.numeric(input$preview_window)
  
  validate(need(!is.na(n_days), "Problem with the Preview Window. Please enter a numeric value."))
  
  dat <- compile_data() 
  
  p <- dat %>% 
    filter(timestamp_utc > max(dat$timestamp_utc) - days(n_days)) %>% 
    ss_ggplot_variables(
      point_size = 1, axis_label_newline = FALSE
    )
  
  p <- ggplotly(p, tooltip = "text", source = "plot2") %>%
    config(
      modeBarButtonsToRemove = list("toggleSpikelines", "autoScale2d"),
      displaylogo = FALSE,
      toImageButtonOptions = list(
        format = "png",
        filename = "deployment_end",
        width = 900, height = 500
      ))  

  p
})

plotlyOutput("p_end")

```


Col {data-height=50}
-------------------------------------

### Print Start Timestamp
```{r}
renderText(click_start())
```

### Print End Timestamp
```{r}
renderText(click_end())
```


Trimmed Data {data-orientation=rows data-icon="fa-scissors"}
=======================================

Col {.tabset}
-------------------------------------

### Trimmed Data 
```{r}

output$p_trim <- renderPlot({
  validate(need(input$apply_trim, "No trimmed data to plot"))
  
  dat <- trim_data()
  
  p <- ss_ggplot_variables(
    dat, axis_label_newline = FALSE, superchill = FALSE
  ) +
    theme(
      text = element_text(size = 16),
      strip.text = element_text(colour = "black", size = 16),
      legend.text = element_text(size = 16),
      legend.key.height = unit(1.5, "cm")
    )
  
  p
})

plotOutput("p_trim")

```

### Data 
```{r}
renderDataTable({
  validate(need(input$apply_trim, "No trimmed data"))
  
  download_name <- paste0(get_folder_name(), "_trimmed")
  
  trim_data() %>%
    mutate(timestamp_utc = format(timestamp_utc)) %>% 
    datatable(
      rownames = FALSE,
      options = list(
         dom = 'ftpl',
        #dom = 'ftpli',
        #dom = 'ft',
        paging = TRUE,
        searching = TRUE,
        scrollX = "500px",
        fillContainer = TRUE,
        columnDefs = list(list(className = 'dt-center', targets = "_all"))
      )
    )
})
```





